{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHWiHxPRHnGZ"
   },
   "source": [
    "# Colaboratoryで実行する場合\n",
    "以下を実行して、外部ファイルをダウンロードしてください。   \n",
    "**このセルはColaboratoryを起動するたびに必要となります**   \n",
    "**<font color='red'>和文フォントをインストールしています。以下のセルを実行後、ランタイムを再起動してください。</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ecYCiyjHnGa",
    "outputId": "6537219c-cb04-445e-bfcb-ea7d29079aad"
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "### Colaboratoryのみ以下を実行 ###\n",
    "##################################\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !wget -P ./text http://www.hal.t.u-tokyo.ac.jp/~yamakata/lecture/mediaproc/mediaproc4/wiki_wakati.txt\n",
    "    !mkdir model\n",
    "    !mkdir fig\n",
    "    # 以下は日本語フォントをインストールするコマンドです\n",
    "    !apt-get -y install fonts-ipafont-gothic\n",
    "    !rm /root/.cache/matplotlib/*.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6uwr6E1HnGa"
   },
   "source": [
    "# トピック分析３：Word2vecによるWord Embedding\n",
    "\n",
    "ニューラルネットワークによって単語をベクトル化するWord2vecを学びましょう。  \n",
    "ここで、このベクトルとは、その単語の意味が近ければ近く、遠ければ遠くなるようなベクトルです。   \n",
    "また、word2vecで生成されるこのベクトルは、加算・減算ができるということで大きな注目を集めました。   \n",
    "以下のような例が有名です。\n",
    "```\n",
    "king - man + woman = queen\n",
    "```\n",
    "LDAと同じくgensimというライブラリを使って演習します。\n",
    "\n",
    "\n",
    "## 1. モデル学習\n",
    "\n",
    "いま、`text/wiki_wakati.txt`には、基礎演習で用いた6カテゴリからなる2101個のWikipedia記事の本文について、分かち書きされた文が保存されています。  \n",
    "ここに含まれる語彙のそれぞれを`size`で指定されたサイズの次元のベクトルに変換しましょう。   \n",
    "コーパスにあまり現れない単語まで学習しようとすると精度が低くなるので、\n",
    "`mini_count`パラメータを設定して最低20回以上現れる単語に限りましょう。   \n",
    "また、word2vecでは、ある単語の性質を、その前後の単語が何かによって特徴量化していくのですが、このときに考慮する周辺の単語数（窓幅）`window`は5単語とします。つまり、その単語より前の5単語と、後ろの5単語をもって、その単語の特徴とするということです。   \n",
    "窓幅が広すぎると、文全体を学習することになって精度が落ちます。\n",
    "\n",
    "以下のプログラムを実行すると、ピンクの背景で学習時のログが出力されます。   \n",
    "たとえば\n",
    "\n",
    "```\n",
    "2020-06-12 13:20:54,586 : INFO : effective_min_count=20 retains 7668 unique words (11% of original 67458, drops 59790)\n",
    "2020-06-12 13:20:54,588 : INFO : effective_min_count=20 leaves 2217791 word corpus (92% of original 2404119, drops 186328)\n",
    "```\n",
    "は、`mini_count`を20単語にしたことにより、語彙数は59790から7668に削減され、またそれに伴い文中の全2404119単語のうち、92％に相当する2217791単語が学習に使われたことを意味します。   \n",
    "また、その後にEPOCHが1から5まで続いて出力されるでしょう。   \n",
    "ニューラルネットワークでは、まず学習データとして入力ベクトルと出力ベクトルの対を大量に用意します。   \n",
    "また、モデルのパラメータはランダムに設定されます。ですから初期のモデルはでたらめな答えを返すようなモデルです。   \n",
    "学習のフェーズでは、この対を1つずつモデルに渡して、\n",
    "入力ベクトルに対し、対応する出力ベクトルが出力されるようにモデルのパラメータを更新していきます。   \n",
    "このとき、すべての学習データに対して一通りパラメータを更新すればそれで終わりではなく、\n",
    "同じデータを何度も入力して繰り返し学習します。   \n",
    "この各繰り返しをエポックと呼び、今回は全データを5回学習するように設定されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JftJtHa3HnGa",
    "outputId": "24d07ec0-38e1-458d-96bf-ef3116a4e96b"
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "sentences = word2vec.Text8Corpus('text/wiki_wakati.txt')\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, # 入力文の集合\n",
    "                        size=200, # 隠れ層の次元＝embedされた単語ベクトルの次元\n",
    "                        min_count=20, # 出現回数が20単語以下のものは削除\n",
    "                        batch_words=10000,# 全文書を10000単語ごとに分割して学習\n",
    "                        iter = 5, # 全文書を学習する過程を何回繰り返すか(エポック数とも呼ぶ)\n",
    "                        window=15) # 考慮する周辺単語数。ここでは前後5単語\n",
    "\n",
    "model.save('model/wiki.model') # 学習済みモデルを保存しておきましょう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBF7nVvRHnGa"
   },
   "source": [
    "## 2. 単語ベクトルの出力\n",
    "\n",
    "「動物」」の単語ベクトルを出力してみましょう。   \n",
    "これは学習時にsizeで指定した次元数（200次元）になっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cOroM3wNHnGa",
    "outputId": "78eefd0c-1aa0-400f-eb7f-397e5f331332"
   },
   "outputs": [],
   "source": [
    "print(model.wv['動物'])\n",
    "len(model.wv['動物'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08sqfiGSHnGa"
   },
   "source": [
    "## 3. 単語ベクトルの活用\n",
    "### 3.1 似た意味を持つ単語をみつけよう\n",
    "\n",
    "単語ベクトル同士が近ければ、その単語の意味は似ていると考えられます。   \n",
    "ここで、似たベクトルをもつ単語を出力してみましょう。\n",
    "\n",
    "単語`sample`をいろいろと変えて似た単語を表示させてみてください。   \n",
    "ただし、出現回数がmini_count以下の単語は学習の段階で無視されるので、\n",
    "単語ベクトルも計算されていません。   \n",
    "そのような単語を指定すると\n",
    "```\n",
    "KeyError: \"word '語彙' not in vocabulary\"\n",
    "```\n",
    "というエラーが出るので注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNZa3zC1HnGa",
    "outputId": "949a3354-d82f-4ee6-965c-94723d35cec8"
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "sample = '動物'\n",
    "#sample = '日本'\n",
    "#sample = 'カメラ'\n",
    "\n",
    "#model = word2vec.Word2Vec.load(\"model/wiki.model\") # 学習済みモデルを読み込む場合\n",
    "results = model.wv.most_similar(positive=[sample])\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxNjENWBHnGa"
   },
   "source": [
    "### 3.2 ベクトルの加算・減算\n",
    "\n",
    "Word2vecは、複数の単語ベクトルを加算・減算したときに、その意味に相当するような単語を見つけることができるという点で大きな話題を呼びました。   \n",
    "今回はコーパスが小さいのであまりうまく行かないかもしれませんが、いろいろと試してみてください。   \n",
    "（学習データが動物、植物、政治、経済、法、芸術の6カテゴリの記事であることを考慮してください。その中で`mini_count`回以上現れる語彙でないと計算できません！）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ZbWe-U8HnGa",
    "outputId": "0d1efeaf-c5c2-4a9e-e09c-67dda47a01c1"
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# 「国会」-「日本」+「米国」=「下院」\n",
    "#sample1 = '国会'\n",
    "#sample2 = '日本'\n",
    "#sample3 = '米国' \n",
    "\n",
    "# 「生育」-「植物」+「動物」=「飼育」\n",
    "#sample1 = '生育'\n",
    "#sample2 = '植物'\n",
    "#sample3 = '動物'\n",
    "\n",
    "# 「ロンドン」-「イギリス」+「フランス」=「パリ」\n",
    "#sample1 = 'ロンドン'\n",
    "#sample2 = 'イギリス'\n",
    "#sample3 = 'フランス'\n",
    "\n",
    "# 「ロンドン」-「イギリス」+「日本」=「東京」\n",
    "sample1 = 'ロンドン'\n",
    "sample2 = 'イギリス'\n",
    "sample3 = '日本'\n",
    "\n",
    "model = word2vec.Word2Vec.load(\"model/wiki.model\")\n",
    "results = model.wv.most_similar(positive=[sample3, sample1], negative=[sample2])\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gq8v-ntjHnGa"
   },
   "source": [
    "### 3.3 単語ベクトルの可視化\n",
    "\n",
    "今回学習に用いた6種類のカテゴリに属する単語のうち、各カテゴリの重要語について、それらのベクトルがどのように分布しているか調べてみましょう。   \n",
    "各単語は200次元のベクトルになりますから、これを2次元平面上にプロットするために、\n",
    "主成分分析（PCA: Principal Component Analysis)を使って2次元に圧縮しています。   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExOIef1CHnGa",
    "outputId": "7994afb7-bd16-41e4-f466-87db826a6a3a"
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "#import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "'''\n",
    "b\t青 (Blue)\n",
    "g\t緑 (Green)\n",
    "r\t赤 (Red)\n",
    "c\tシアン (Cyan)\n",
    "m\tマゼンタ (Magenta)\n",
    "y\t黄 (Yellow)\n",
    "k\t黒 (Black)\n",
    "w\t白 (White)\n",
    "'''\n",
    "# 単語のリストを設定します\n",
    "# このとき、可視化したさいに同じカテゴリの重要語が同じ色で塗られるよう、\n",
    "# 同じカテゴリの単語に同じカラーコード[bgrcmy]を割り当てておきます\n",
    "words = {'動物':'b','種':'b', '細胞':'b', '生物':'b', '卵':'b','寄生':'b', '飼育':'b', '虫':'b', '家畜':'b', '犬':'b', # animal\n",
    "         '作品':'g', '写真':'g', 'アニメ':'g', 'カメラ':'g', '音楽':'g', 'テレビ':'g', 'スタジオ':'g', '漫画':'g', '表現':'g', '賞':'g', # art\n",
    "          '経済':'r', '企業':'r', '社会':'r', '資本':'r', '会社':'r', '生産':'r', '労働':'r', '産業':'r', '市場':'r', '価格':'r', # economy\n",
    "          '法':'c', '憲法':'c', '国家':'c', '監査':'c', '権利':'c', '解釈':'c', '法律':'c', '行政':'c', '教会':'c', '宗教':'c', #law\n",
    "         '植物':'m', '葉':'m', '栽培':'m', '品種':'m', '茎':'m', '種子':'m', '枝':'m', '根':'m', '胞子':'m', '果実':'m', #plant\n",
    "         '軍事':'y', '選挙':'y', '国民':'y', '議会':'y', '主権':'y', '地方':'y', '外交':'y', '行政':'y', '権力':'y', '期間':'y'} #politic\n",
    "\n",
    "# 各単語についてword2vecの学習モデルによって200次元のベクトルを獲得します\n",
    "model = word2vec.Word2Vec.load(\"model/wiki.model\")\n",
    "data = []\n",
    "for word in words.keys():\n",
    "    data.append(model.wv[word])\n",
    "\n",
    "# 200次元を2次元に圧縮するためPCAを用いています\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(data)\n",
    "data_pca= pca.transform(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppvYi-JnHnGa"
   },
   "source": [
    "単語の分布を可視化してみましょう。   \n",
    "同じカテゴリに属する単語は近くに分布しているでしょうか？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "id": "hlftENRKHnGa",
    "outputId": "bf8ae9d5-6744-4910-cc49-12f6a206bc28"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# IPAex日本語フォントのインストールができた人は、以下によって日本語が出力できるはずです\n",
    "# できていない環境では、日本語の文字は□に置き換えられます\n",
    "#plt.rcParams[\"font.family\"] = \"IPAexGothic\"\n",
    "\n",
    "# Windowsなら以下でうまく行くかもしれません\n",
    "# plt.rcParams[\"font.family\"] = \"Yu Mincho\"\n",
    "# Macなら以下でうまく行くかもしれません\n",
    "# plt.rcParams['font.family'] = 'AppleGothic'\n",
    "# Colaboratoryでは以下を指定してください\n",
    "plt.rcParams[\"font.family\"] = \"IPAGothic\"\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 18\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "num = 0\n",
    "for word in words:\n",
    "    plt.plot(data_pca[num][0], data_pca[num][1], ms=5.0, zorder=2, marker='x', color=words[word])\n",
    "    plt.annotate(word, (data_pca[num][0], data_pca[num][1])) \n",
    "    num += 1\n",
    "plt.grid()\n",
    "plt.savefig('fig/Word2vec1.png') # 図を画像として保存"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TopicAnalysis3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
